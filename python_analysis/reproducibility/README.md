# Reproducibility Framework for Regge Analysis

This directory contains the reproducibility framework for the Regge trajectory analysis, ensuring that all results can be exactly replicated by others.

## ğŸ¯ Overview

The reproducibility framework provides:

- **Deterministic Data Assembly**: Frozen CSV snapshots with metadata
- **Parameterized Analysis**: Configurable, headless execution
- **Comprehensive Testing**: Unit tests and validation checks
- **CI/CD Integration**: Automated testing and validation
- **Cross-Platform Validation**: Wolfram Language and Python comparison

## ğŸ“ Directory Structure

```
reproducibility/
â”œâ”€â”€ data_assembly.py          # Deterministic data assembly
â”œâ”€â”€ run_regge_analysis.py     # Parameterized analysis runner
â”œâ”€â”€ test_regge_analysis.py    # Testing framework
â””â”€â”€ README.md                 # This file
```

## ğŸš€ Quick Start

### 1. Create Frozen Data Snapshots

```python
from reproducibility.data_assembly import DeterministicDataAssembler

# Create assembler
assembler = DeterministicDataAssembler(output_dir="data_snapshots")

# Create frozen snapshot for Delta baryons
snapshot_path = assembler.create_frozen_snapshot(
    particle_family="Delta",
    include_width_systematic=True,
    kappa=0.25
)

print(f"Created snapshot: {snapshot_path}")
```

### 2. Run Parameterized Analysis

```python
from reproducibility.run_regge_analysis import ParameterizedReggeAnalyzer

# Create analyzer with settings
analyzer = ParameterizedReggeAnalyzer("settings.json")

# Run complete analysis
results = analyzer.run_analysis()

print(f"Analysis completed: {results['validation']['passed']}")
```

### 3. Run Tests

```python
from reproducibility.test_regge_analysis import run_ci_tests

# Run all tests
success = run_ci_tests()

if success:
    print("âœ… All tests passed!")
else:
    print("âŒ Some tests failed!")
```

## ğŸ“‹ Detailed Usage

### Deterministic Data Assembly

The `DeterministicDataAssembler` creates frozen CSV snapshots with complete metadata:

```python
assembler = DeterministicDataAssembler()

# Assemble data for different particle families
delta_data = assembler.assemble_pdg_data("Delta", include_width_systematic=True, kappa=0.25)
nstar_data = assembler.assemble_pdg_data("Nstar", include_width_systematic=True, kappa=0.25)

# Create frozen snapshots
delta_snapshot = assembler.create_frozen_snapshot("Delta")
nstar_snapshot = assembler.create_frozen_snapshot("Nstar")

# Load frozen snapshots
data, metadata = assembler.load_frozen_snapshot(delta_snapshot)
```

**Features:**
- Standardized units (GeV)
- Proper uncertainty propagation
- Width-based systematic uncertainties
- Complete metadata tracking
- Version control for PDG data

### Parameterized Analysis Runner

The `ParameterizedReggeAnalyzer` provides configurable, reproducible analysis:

```python
# Create settings
settings = {
    'data': {
        'particle_family': 'Delta',
        'include_width_systematic': True,
        'kappa': 0.25,
        'use_frozen_snapshot': False
    },
    'analysis': {
        'bootstrap_n': 1000,
        'window_factor': 2.0,
        'canonical_alpha0': 0.0,
        'canonical_alphap': 0.9,
        'J_range': [0.5, 9.5],
        'J_step': 0.5
    },
    'output': {
        'output_dir': 'analysis_results',
        'save_plots': True,
        'save_reports': True
    },
    'validation': {
        'run_tests': True,
        'sanity_bounds': {
            'alphap_min': 0.1,
            'alphap_max': 2.0,
            'chi2_dof_max': 10.0
        }
    }
}

# Save settings to file
import json
with open('settings.json', 'w') as f:
    json.dump(settings, f)

# Run analysis
analyzer = ParameterizedReggeAnalyzer('settings.json')
results = analyzer.run_analysis()
```

**Features:**
- Configurable via JSON settings
- Deterministic execution
- Comprehensive validation
- Automated report generation
- Error handling and logging

### Testing Framework

The testing framework provides comprehensive validation:

```python
from reproducibility.test_regge_analysis import TestReggeAnalysis, run_ci_tests

# Run individual tests
test_suite = TestReggeAnalysis()
test_suite.setUp()

# Test data assembly
test_suite.test_data_assembly()

# Test Regge fitting
test_suite.test_regge_fitter()

# Test bootstrap analysis
test_suite.test_bootstrap_analysis()

# Test validation analysis
test_suite.test_validation_analysis()

# Test parameterized analysis
test_suite.test_parameterized_analysis()

# Test reproducibility
test_suite.test_reproducibility()

# Clean up
test_suite.tearDown()

# Run all tests for CI
success = run_ci_tests()
```

**Test Coverage:**
- Data assembly and validation
- Regge trajectory fitting (WLS, ODR)
- Bootstrap analysis
- Validation analysis
- Parameterized analysis runner
- Reproducibility checks
- Error handling and edge cases
- Data validation

## ğŸ”§ Command Line Interface

### Create Settings Template

```bash
python run_regge_analysis.py --create-template
```

This creates a `settings_template.json` file that you can customize.

### Run Analysis with Command Line Arguments

```bash
# Basic analysis
python run_regge_analysis.py --particle-family Delta --bootstrap-n 1000

# Use custom settings file
python run_regge_analysis.py --settings my_settings.json

# Override settings
python run_regge_analysis.py --particle-family Nstar --output-dir my_results
```

### Run Tests

```bash
# Run all tests
python test_regge_analysis.py

# Run with verbose output
python -m pytest test_regge_analysis.py -v
```

## ğŸ“Š Output Files

The analysis generates comprehensive output:

### Data Snapshots
- `{family}_baryons_pdg{version}_{timestamp}.csv` - Frozen data snapshot
- `{family}_baryons_pdg{version}_{timestamp}.json` - Metadata

### Analysis Results
- `analysis_settings.json` - Complete settings used
- `analysis_summary.json` - Key results summary
- `reproducibility_report.txt` - Reproducibility report
- `parameter_report.txt` - Parameter report
- `validation_report.txt` - Validation report

### Plots and Visualizations
- `regge_fit.png` - Main Regge trajectory fit
- `bootstrap_distributions.png` - Bootstrap analysis
- `predictions.png` - Predicted missing states
- `validation_analysis.png` - Validation analysis
- `statistical_robustness.png` - Statistical significance
- `theoretical_analysis.png` - Theoretical context

### Data Files
- `predictions.csv` - Predicted missing J states
- `cross_check_results.csv` - PDG cross-check results
- `method_comparison.csv` - Method comparison results

## ğŸ”„ CI/CD Integration

The framework includes GitHub Actions for continuous integration:

```yaml
# .github/workflows/test_regge_analysis.yml
name: Test Regge Analysis

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"
      - name: Install dependencies
        run: pip install -r requirements.txt
      - name: Run tests
        run: python test_regge_analysis.py
```

**CI Features:**
- Multi-Python version testing (3.8, 3.9, 3.10, 3.11)
- Automated reproducibility tests
- Data assembly validation
- Result validation
- Comprehensive reporting

## ğŸ“ˆ Validation Checks

The framework performs comprehensive validation:

### Data Validation
- âœ… Data row counts > 0
- âœ… No missing values in required fields
- âœ… Mass range within reasonable bounds
- âœ… J values non-negative
- âœ… Uncertainty propagation correct

### Analysis Validation
- âœ… Î±' within sanity bounds (0.1 - 2.0 GeVâ»Â²)
- âœ… Ï‡Â²/dof within reasonable bounds
- âœ… Bootstrap variance non-zero
- âœ… Predictions sorted by J
- âœ… Parameter uncertainties positive

### Reproducibility Validation
- âœ… Deterministic data assembly
- âœ… Parameter reproducibility within tolerance
- âœ… Cross-platform consistency
- âœ… Version control compliance

## ğŸ”— Cross-Platform Validation

The framework supports comparison between Wolfram Language and Python:

```python
from data_export.wl_reproducibility_guide import compare_wl_python_reproducibility

# Compare results
comparison = compare_wl_python_reproducibility(
    "wl_reproducibility_results.json",
    python_results
)

print(f"Agreement: {comparison['analysis_results']['agreement']}")
```

## ğŸ“ Paper Integration

For paper submission, include:

### Methods Section
```
Data Assembly: We used deterministic data assembly with frozen CSV snapshots 
(PDG 2024, timestamp: 2024-12-01T14:30:22). All data processing was performed 
with standardized units (GeV) and proper uncertainty propagation including 
width-based systematic uncertainties (Îº = 0.25).

Analysis Pipeline: The complete analysis pipeline was parameterized and 
executed using reproducible settings (see Supplementary Materials). All 
results were validated against sanity bounds and tested for reproducibility.

Code Availability: Complete code and data are available at [repository URL] 
with build badge indicating CI/CD validation.
```

### Reproducibility Statement
```
Reproducibility: All analysis results can be exactly reproduced using the 
provided frozen data snapshots and parameterized analysis scripts. The 
complete pipeline has been validated through automated testing and CI/CD 
integration. Results are reproducible within numerical precision (tolerance: 
1Ã—10â»â¶) across multiple runs and platforms.
```

## ğŸ› ï¸ Troubleshooting

### Common Issues

1. **Import Errors**: Ensure all dependencies are installed
   ```bash
   pip install -r requirements.txt
   ```

2. **Data Assembly Failures**: Check PDG data availability and format

3. **Validation Failures**: Review sanity bounds and adjust if needed

4. **Reproducibility Issues**: Check for non-deterministic operations

### Debug Mode

Enable debug output:
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

### Validation Reports

Review validation reports for detailed error information:
```bash
cat analysis_results/validation_report.txt
```

## ğŸ“š References

- Particle Data Group (PDG) 2024 Review
- Regge trajectory phenomenology
- Statistical analysis best practices
- Reproducible research standards

## ğŸ¤ Contributing

1. Follow the existing code structure
2. Add tests for new features
3. Update documentation
4. Ensure reproducibility
5. Validate cross-platform compatibility

## ğŸ“„ License

This reproducibility framework is part of the WSRP25 project and follows the same licensing terms.

---

**Build Status**: ![CI/CD](https://github.com/your-repo/wsrp/workflows/Test%20Regge%20Analysis/badge.svg)

**Last Updated**: December 2024
